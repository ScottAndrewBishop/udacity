{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import torch\n",
    "import numpy               as np\n",
    "import torch.nn            as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim         as optim\n",
    "\n",
    "from unityagents import UnityEnvironment\n",
    "from collections import (\n",
    "    deque, \n",
    "    namedtuple\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_size, action_size, fc1, fc2, leak):\n",
    "        super(Actor, self).__init__()\n",
    "        \n",
    "        self.seed = torch.manual_seed(SEED)\n",
    "        self.leak = leak\n",
    "        self.bn   = nn.BatchNorm1d(state_size)\n",
    "\n",
    "        self.fc1 = nn.Linear(state_size,         fc1)\n",
    "        self.fc2 = nn.Linear(       fc1,         fc2)\n",
    "        self.fc3 = nn.Linear(       fc2, action_size)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.kaiming_normal_(self.fc1.weight.data, a = self.leak, mode = 'fan_in')\n",
    "        torch.nn.init.kaiming_normal_(self.fc2.weight.data, a = self.leak, mode = 'fan_in')\n",
    "        \n",
    "        torch.nn.init.uniform_(self.fc3.weight.data, -3e-3, 3e-3)\n",
    "\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = F.leaky_relu(self.fc1(self.bn(state)), negative_slope = self.leak)\n",
    "        x = F.leaky_relu(self.fc2(x),              negative_slope = self.leak)\n",
    "        \n",
    "        return torch.tanh(self.fc3(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_size, action_size, fc1, fc2, fc3, leak ):\n",
    "        super(Critic, self).__init__()\n",
    "        \n",
    "        self.seed = torch.manual_seed(SEED)\n",
    "        self.leak = leak\n",
    "        self.bn   = nn.BatchNorm1d(state_size)\n",
    "        \n",
    "        self.fc1 = nn.Linear(       state_size, fc1)\n",
    "        self.fc2 = nn.Linear(fc1 + action_size, fc2)\n",
    "        self.fc3 = nn.Linear(              fc2, fc3)\n",
    "        self.fc4 = nn.Linear(              fc3,   1)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.kaiming_normal_(self.fc1.weight.data, a = self.leak, mode = 'fan_in')\n",
    "        torch.nn.init.kaiming_normal_(self.fc2.weight.data, a = self.leak, mode = 'fan_in')\n",
    "        \n",
    "        torch.nn.init.uniform_(self.fc3.weight.data, -3e-3, 3e-3)\n",
    "\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        x = F.leaky_relu(self.fc1(self.bn(state)), negative_slope = self.leak)\n",
    "        \n",
    "        x = torch.cat((x, action), dim = 1)\n",
    "        \n",
    "        x = F.leaky_relu(self.fc2(x), negative_slope = self.leak)\n",
    "        x = F.leaky_relu(self.fc3(x), negative_slope = self.leak)\n",
    "        \n",
    "        return self.fc4(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUNoise:\n",
    "    def __init__(self, size, mu, theta, sigma):\n",
    "        self.seed = np.random.seed(SEED)\n",
    "        random.seed(SEED)\n",
    "        \n",
    "        self.mu    = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        \n",
    "        self.reset()\n",
    "        \n",
    "        \n",
    "    def reset(self):\n",
    "        self.state = copy.copy(self.mu)\n",
    "        \n",
    "        \n",
    "    def sample(self):\n",
    "        x          = self.state\n",
    "        dx         = self.theta * (self.mu - x) + self.sigma * np.array([np.random.random() for i in range(len(x))])\n",
    "        self.state = x + dx\n",
    "        \n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, action_size, buffer_size, batch_size, device):\n",
    "        self.seed = np.random.seed(SEED)\n",
    "        random.seed(SEED)\n",
    "        \n",
    "        self.device      = device\n",
    "        self.action_size = action_size\n",
    "        self.memory      = deque(maxlen = buffer_size) \n",
    "        self.batch_size  = batch_size\n",
    "        self.experience  = namedtuple(\n",
    "            \"Experience\", \n",
    "            field_names = [\n",
    "                \"state\", \n",
    "                \"action\", \n",
    "                \"reward\", \n",
    "                \"next_state\", \n",
    "                \"done\"\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        exp = self.experience(state, action, reward, next_state, done)\n",
    "        \n",
    "        self.memory.append(exp)\n",
    "\n",
    "        \n",
    "    def sample(self):\n",
    "        experiences = random.sample(self.memory, k = self.batch_size)\n",
    "       \n",
    "        return (\n",
    "            torch.from_numpy(np.vstack([e.state      for e in experiences if e is not None])                 ).float().to(self.device),\n",
    "            torch.from_numpy(np.vstack([e.action     for e in experiences if e is not None])                 ).float().to(self.device),\n",
    "            torch.from_numpy(np.vstack([e.reward     for e in experiences if e is not None])                 ).float().to(self.device),\n",
    "            torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])                 ).float().to(self.device),\n",
    "            torch.from_numpy(np.vstack([e.done       for e in experiences if e is not None]).astype(np.uint8)).float().to(self.device)\n",
    "        )\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, state_size, action_size, n_agents, fc1, fc2, fc3, leakiness, actor_lr, critic_lr, buffer_size, batch_size, gamma, tau, decay, mu, theta, sigma):\n",
    "        self.seed = np.random.seed(SEED)\n",
    "        random.seed(SEED)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            self.device = \"cuda:0\"\n",
    "            print(f\"[INFO] training on CUDA\")\n",
    "            \n",
    "        else:\n",
    "            self.device = \"cpu\"\n",
    "            print(f\"[INFO] training on CPU\")\n",
    "            \n",
    "        self.state_size  = state_size\n",
    "        self.action_size = action_size\n",
    "        self.n_agents    = n_agents\n",
    "        self.fc1         = fc1\n",
    "        self.fc2         = fc2\n",
    "        self.fc3         = fc3\n",
    "        self.leakiness   = leakiness\n",
    "        self.actor_lr    = actor_lr\n",
    "        self.critic_lr   = critic_lr\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size  = batch_size\n",
    "        self.gamma       = gamma\n",
    "        self.tau         = tau\n",
    "        self.decay       = decay\n",
    "        self.mu          = mu\n",
    "        self.theta       = theta\n",
    "        self.sigma       = sigma\n",
    "\n",
    "        \n",
    "        # Actor\n",
    "        # --------------------------------------------------\n",
    "        self.actor_local  = Actor(self.state_size, self.action_size, self.fc1, self.fc2, self.leakiness).to(self.device)\n",
    "        self.actor_target = Actor(self.state_size, self.action_size, self.fc1, self.fc2, self.leakiness).to(self.device)\n",
    "        \n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr = self.actor_lr)\n",
    "\n",
    "        \n",
    "        # Critic\n",
    "        # --------------------------------------------------\n",
    "        self.critic_local  = Critic(self.state_size, self.action_size, self.fc1, self.fc2, self.fc3, self.leakiness).to(self.device)\n",
    "        self.critic_target = Critic(self.state_size, self.action_size, self.fc1, self.fc2, self.fc3, self.leakiness).to(self.device)\n",
    "        \n",
    "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr = self.critic_lr)\n",
    "\n",
    "        \n",
    "        # Noise\n",
    "        # --------------------------------------------------\n",
    "        self.noise = OUNoise(self.action_size, self.mu, self.theta, self.sigma)\n",
    "\n",
    "        \n",
    "        # Replay Buffer\n",
    "        # --------------------------------------------------\n",
    "        self.memory    = ReplayBuffer(self.action_size, self.buffer_size, self.batch_size, self.device)\n",
    "        self.timesteps = 0  \n",
    "\n",
    "        \n",
    "    def step(self, states, actions, rewards, next_states, dones):\n",
    "        self.timesteps += 1\n",
    "        \n",
    "        for i in range(self.n_agents):\n",
    "            self.memory.add(\n",
    "                states[i], \n",
    "                actions[i], \n",
    "                rewards[i], \n",
    "                next_states[i], \n",
    "                dones[i]\n",
    "            )\n",
    "\n",
    "            \n",
    "        if (len(self.memory) > self.batch_size) and (self.timesteps % self.n_agents == 0):\n",
    "            for _ in range(10):\n",
    "                exp = self.memory.sample()\n",
    "                \n",
    "                self.learn(exp)\n",
    "\n",
    "                \n",
    "    def act(self, states):\n",
    "        states = torch.from_numpy(states).float().to(self.device)\n",
    "        \n",
    "        self.actor_local.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            actions = self.actor_local(states).cpu().data.numpy()\n",
    "            \n",
    "        self.actor_local.train()\n",
    "        \n",
    "        actions += [self.noise.sample() for _ in range(self.n_agents)]\n",
    "            \n",
    "        return np.clip(actions, -1, 1)\n",
    "\n",
    "    \n",
    "    def reset(self):\n",
    "        self.noise.reset()\n",
    "\n",
    "        \n",
    "    def learn(self, experiences):\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        \n",
    "        # Critic Update\n",
    "        # --------------------------------------------------\n",
    "        actions_next = self.actor_target(next_states)\n",
    "        \n",
    "        Q_targets_next = self.critic_target(next_states, actions_next)\n",
    "        Q_targets      = rewards + (self.gamma * Q_targets_next * (1 - dones))\n",
    "        Q_expected     = self.critic_local(states, actions)\n",
    "        \n",
    "        critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        \n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        \n",
    "        # Actor Update\n",
    "        # --------------------------------------------------\n",
    "        actions_pred =  self.actor_local(states)\n",
    "        actor_loss   = -self.critic_local(states, actions_pred).mean()\n",
    "\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        \n",
    "        # Target Update\n",
    "        # --------------------------------------------------\n",
    "        self.soft_update(self.critic_local, self.critic_target)\n",
    "        self.soft_update(self.actor_local,  self.actor_target)\n",
    "\n",
    "        \n",
    "    def soft_update(self, local_model, target_model):\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(self.tau * local_param.data + (1.0 - self.tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Envorinment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "# Environement \n",
    "# --------------------------------------------------\n",
    "SEED   = 42\n",
    "ENV_FP = f\"Reacher_Windows_x86_64/Reacher.exe\"\n",
    "ENV    = UnityEnvironment(file_name = ENV_FP)\n",
    "\n",
    "\n",
    "# Brain \n",
    "# --------------------------------------------------\n",
    "BRAIN_NAME = ENV.brain_names[0]\n",
    "BRAIN      = ENV.brains[BRAIN_NAME]\n",
    "\n",
    "\n",
    "# Environment Data \n",
    "# --------------------------------------------------\n",
    "ENV_INFO    = ENV.reset(train_mode = True)[BRAIN_NAME]\n",
    "N_AGENTS    = len(ENV_INFO.agents)\n",
    "ACTION_SIZE = BRAIN.vector_action_space_size\n",
    "STATE_SIZE  = ENV_INFO.vector_observations.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training on CUDA\n"
     ]
    }
   ],
   "source": [
    "AGENT = Agent(\n",
    "    state_size  = STATE_SIZE, \n",
    "    action_size = ACTION_SIZE, \n",
    "    n_agents    = N_AGENTS,\n",
    "    fc1         = 256, \n",
    "    fc2         = 128, \n",
    "    fc3         = 128,\n",
    "    leakiness   = 1e-2,\n",
    "    actor_lr    = 1e-4,\n",
    "    critic_lr   = 3e-4,\n",
    "    buffer_size = 1000000,\n",
    "    batch_size  = 1024,\n",
    "    gamma       = 0.99,\n",
    "    tau         = 1e-3,\n",
    "    decay       = 1e-4,\n",
    "    mu          = 0.0, \n",
    "    theta       = 0.15, \n",
    "    sigma       = 0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Training Scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddpg(n_episodes = 2000, max_t = 1000, epochs = 1000):\n",
    "    window_size        = 100\n",
    "    scores_deque       = deque(maxlen = window_size) \n",
    "    scores             = []        \n",
    "    best_average_score = -np.inf\n",
    "    \n",
    "    for i_episode in range(1, epochs + 1):\n",
    "        env_info       = ENV.reset(train_mode = True)[BRAIN_NAME]\n",
    "        states         = env_info.vector_observations\n",
    "        episode_scores = np.zeros(N_AGENTS) \n",
    "        \n",
    "        AGENT.reset()\n",
    "\n",
    "        for t in range(max_t):\n",
    "            actions     = AGENT.act(states)\n",
    "            env_info    = ENV.step(actions)[BRAIN_NAME]\n",
    "            next_states = env_info.vector_observations\n",
    "            rewards     = env_info.rewards\n",
    "            dones       = env_info.local_done\n",
    "\n",
    "            AGENT.step(\n",
    "                states      = states, \n",
    "                actions     = actions, \n",
    "                rewards     = rewards, \n",
    "                next_states = next_states, \n",
    "                dones       = dones\n",
    "            )\n",
    "            \n",
    "            episode_scores += np.array(rewards)\n",
    "            states          = next_states\n",
    "            \n",
    "            if np.any(dones):\n",
    "                break\n",
    "\n",
    "        episode_score = np.mean(episode_scores)\n",
    "        \n",
    "        scores_deque.append(episode_score)\n",
    "        scores.append(episode_score)\n",
    "        \n",
    "        average_score = np.mean(scores_deque)\n",
    "        \n",
    "        print('\\rEpisode: {}\\tAverage Score: {:.2f}\\tCurrent Score: {:.2f}'.format(i_episode, average_score, episode_score), end=\"\")\n",
    "        \n",
    "        if i_episode % 10 == 0:\n",
    "            print('\\rEpisode: {}\\tAverage Score: {:.2f}\\tCurrent Score: {:.2f}'.format(i_episode, average_score, episode_score))\n",
    "\n",
    "        if average_score >= 30.0:\n",
    "            print('\\nEnvironment solved in {} episodes!\\tAverage Score: {:.2f}'.format(i_episode - window_size, average_score))\n",
    "            \n",
    "            torch.save(AGENT.actor_local.state_dict(),  os.path.join(\"checkpoints\", \"checkpoint_actor.pth\")) \n",
    "            torch.save(AGENT.critic_local.state_dict(), os.path.join(\"checkpoints\", \"checkpoint_critic.pth\"))\n",
    "            \n",
    "            break\n",
    "\n",
    "    np.save('scores/scores.npy', scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 10\tAverage Score: 0.48\tCurrent Score: 0.69\n",
      "Episode: 20\tAverage Score: 0.68\tCurrent Score: 0.96\n",
      "Episode: 30\tAverage Score: 0.87\tCurrent Score: 1.60\n",
      "Episode: 40\tAverage Score: 1.21\tCurrent Score: 2.72\n",
      "Episode: 50\tAverage Score: 1.68\tCurrent Score: 4.61\n",
      "Episode: 60\tAverage Score: 2.29\tCurrent Score: 5.50\n",
      "Episode: 70\tAverage Score: 3.02\tCurrent Score: 8.87\n",
      "Episode: 80\tAverage Score: 3.82\tCurrent Score: 9.694\n",
      "Episode: 90\tAverage Score: 4.64\tCurrent Score: 13.69\n",
      "Episode: 100\tAverage Score: 5.64\tCurrent Score: 15.26\n",
      "Episode: 110\tAverage Score: 7.34\tCurrent Score: 19.34\n",
      "Episode: 120\tAverage Score: 9.45\tCurrent Score: 23.88\n",
      "Episode: 130\tAverage Score: 11.87\tCurrent Score: 25.98\n",
      "Episode: 140\tAverage Score: 14.56\tCurrent Score: 30.38\n",
      "Episode: 150\tAverage Score: 17.19\tCurrent Score: 29.13\n",
      "Episode: 160\tAverage Score: 19.83\tCurrent Score: 31.06\n",
      "Episode: 170\tAverage Score: 22.18\tCurrent Score: 31.52\n",
      "Episode: 180\tAverage Score: 24.33\tCurrent Score: 28.48\n",
      "Episode: 190\tAverage Score: 26.22\tCurrent Score: 31.48\n",
      "Episode: 200\tAverage Score: 28.00\tCurrent Score: 33.01\n",
      "Episode: 210\tAverage Score: 29.43\tCurrent Score: 31.00\n",
      "Episode: 216\tAverage Score: 30.02\tCurrent Score: 30.50\n",
      "Environment solved in 116 episodes!\tAverage Score: 30.02\n"
     ]
    }
   ],
   "source": [
    "ddpg(\n",
    "    n_episodes = 2000, \n",
    "    max_t      = 1000, \n",
    "    epochs     = 1000\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

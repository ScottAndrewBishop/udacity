{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import torch\n",
    "import numpy               as np\n",
    "import torch.nn            as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim         as optim\n",
    "\n",
    "from unityagents import UnityEnvironment\n",
    "from collections import (\n",
    "    deque, \n",
    "    namedtuple\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_size, action_size, fc1, fc2, leak):\n",
    "        super(Actor, self).__init__()\n",
    "        \n",
    "        self.seed = torch.manual_seed(SEED)\n",
    "        self.leak = leak\n",
    "        self.bn   = nn.BatchNorm1d(state_size)\n",
    "\n",
    "        self.fc1 = nn.Linear(state_size,         fc1)\n",
    "        self.fc2 = nn.Linear(       fc1,         fc2)\n",
    "        self.fc3 = nn.Linear(       fc2, action_size)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.kaiming_normal_(self.fc1.weight.data, a = self.leak, mode = 'fan_in')\n",
    "        torch.nn.init.kaiming_normal_(self.fc2.weight.data, a = self.leak, mode = 'fan_in')\n",
    "        \n",
    "        torch.nn.init.uniform_(self.fc3.weight.data, -3e-3, 3e-3)\n",
    "\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = F.leaky_relu(self.fc1(self.bn(state)), negative_slope = self.leak)\n",
    "        x = F.leaky_relu(self.fc2(x),              negative_slope = self.leak)\n",
    "        \n",
    "        return torch.tanh(self.fc3(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_size, action_size, fc1, fc2, leak):\n",
    "        super(Critic, self).__init__()\n",
    "        \n",
    "        self.seed = torch.manual_seed(SEED)\n",
    "        self.leak = leak\n",
    "        self.bn   = nn.BatchNorm1d(state_size)\n",
    "        \n",
    "        self.fc1 = nn.Linear(       state_size, fc1)\n",
    "        self.fc2 = nn.Linear(fc1 + action_size, fc2)\n",
    "        self.fc3 = nn.Linear(              fc2,   1)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.kaiming_normal_(self.fc1.weight.data, a = self.leak, mode = 'fan_in')\n",
    "        torch.nn.init.kaiming_normal_(self.fc2.weight.data, a = self.leak, mode = 'fan_in')\n",
    "        \n",
    "        torch.nn.init.uniform_(self.fc3.weight.data, -3e-3, 3e-3)\n",
    "\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        x = F.leaky_relu(self.fc1(self.bn(state)), negative_slope = self.leak)\n",
    "        x = torch.cat((x, action), dim = 1)\n",
    "        x = F.leaky_relu(self.fc2(x), negative_slope = self.leak)\n",
    "        \n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUNoise:\n",
    "    def __init__(self, size, mu, theta, sigma):\n",
    "        self.seed = np.random.seed(SEED)\n",
    "        random.seed(SEED)\n",
    "        \n",
    "        self.mu    = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        \n",
    "        self.reset()\n",
    "        \n",
    "        \n",
    "    def reset(self):\n",
    "        self.state = copy.copy(self.mu)\n",
    "        \n",
    "        \n",
    "    def sample(self):\n",
    "        x          = self.state\n",
    "        dx         = self.theta * (self.mu - x) + self.sigma * np.array([np.random.normal(self.mu, self.sigma) for i in range(len(x))])\n",
    "        self.state = x + dx\n",
    "        \n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, action_size, buffer_size, batch_size, device):\n",
    "        self.seed = np.random.seed(SEED)\n",
    "        random.seed(SEED)\n",
    "        \n",
    "        self.device      = device\n",
    "        self.action_size = action_size\n",
    "        self.memory      = deque(maxlen = buffer_size) \n",
    "        self.batch_size  = batch_size\n",
    "        self.experience  = namedtuple(\n",
    "            \"Experience\", \n",
    "            field_names = [\n",
    "                \"state\", \n",
    "                \"action\", \n",
    "                \"reward\", \n",
    "                \"next_state\", \n",
    "                \"done\"\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        exp = self.experience(state, action, reward, next_state, done)\n",
    "        \n",
    "        self.memory.append(exp)\n",
    "\n",
    "        \n",
    "    def sample(self):\n",
    "        experiences = random.sample(self.memory, k = self.batch_size)\n",
    "       \n",
    "        return (\n",
    "            torch.from_numpy(np.vstack([e.state      for e in experiences if e is not None])                 ).float().to(self.device),\n",
    "            torch.from_numpy(np.vstack([e.action     for e in experiences if e is not None])                 ).float().to(self.device),\n",
    "            torch.from_numpy(np.vstack([e.reward     for e in experiences if e is not None])                 ).float().to(self.device),\n",
    "            torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])                 ).float().to(self.device),\n",
    "            torch.from_numpy(np.vstack([e.done       for e in experiences if e is not None]).astype(np.uint8)).float().to(self.device)\n",
    "        )\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, state_size, action_size, n_agents, fc1, fc2, leakiness, actor_lr, critic_lr, buffer_size, batch_size, gamma, tau, decay, mu, theta, sigma):\n",
    "        self.seed = np.random.seed(SEED)\n",
    "        random.seed(SEED)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            self.device = \"cuda:0\"\n",
    "            print(f\"[INFO] training on CUDA\")\n",
    "            \n",
    "        else:\n",
    "            self.device = \"cpu\"\n",
    "            print(f\"[INFO] training on CPU\")\n",
    "            \n",
    "        self.state_size  = state_size\n",
    "        self.action_size = action_size\n",
    "        self.n_agents    = n_agents\n",
    "        self.fc1         = fc1\n",
    "        self.fc2         = fc2\n",
    "        self.leakiness   = leakiness\n",
    "        self.actor_lr    = actor_lr\n",
    "        self.critic_lr   = critic_lr\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size  = batch_size\n",
    "        self.gamma       = gamma\n",
    "        self.tau         = tau\n",
    "        self.decay       = decay\n",
    "        self.mu          = mu\n",
    "        self.theta       = theta\n",
    "        self.sigma       = sigma\n",
    "\n",
    "        \n",
    "        # Actor\n",
    "        # --------------------------------------------------\n",
    "        self.actor_local  = Actor(self.state_size, self.action_size, self.fc1, self.fc2, self.leakiness).to(self.device)\n",
    "        self.actor_target = Actor(self.state_size, self.action_size, self.fc1, self.fc2, self.leakiness).to(self.device)\n",
    "        \n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr = self.actor_lr)\n",
    "\n",
    "        \n",
    "        # Critic\n",
    "        # --------------------------------------------------\n",
    "        self.critic_local  = Critic(self.state_size, self.action_size, self.fc1, self.fc2, self.leakiness).to(self.device)\n",
    "        self.critic_target = Critic(self.state_size, self.action_size, self.fc1, self.fc2, self.leakiness).to(self.device)\n",
    "        \n",
    "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr = self.critic_lr)\n",
    "\n",
    "        \n",
    "        # Noise\n",
    "        # --------------------------------------------------\n",
    "        self.noise = OUNoise(self.action_size, self.mu, self.theta, self.sigma)\n",
    "\n",
    "        \n",
    "        # Replay Buffer\n",
    "        # --------------------------------------------------\n",
    "        self.memory = ReplayBuffer(self.action_size, self.buffer_size, self.batch_size, self.device)\n",
    "\n",
    "        \n",
    "    def step(self, states, actions, rewards, next_states, dones, timesteps):            \n",
    "        self.memory.add(\n",
    "            states, \n",
    "            actions, \n",
    "            rewards, \n",
    "            next_states, \n",
    "            dones\n",
    "        )\n",
    "            \n",
    "        if (len(self.memory) > self.batch_size) and (timesteps % self.n_agents == 0):\n",
    "            for _ in range(10):\n",
    "                exp = self.memory.sample()\n",
    "                \n",
    "                self.learn(exp)\n",
    "\n",
    "                \n",
    "    def act(self, states):\n",
    "        states = torch.from_numpy(states).float().to(self.device)\n",
    "        \n",
    "        self.actor_local.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            actions = self.actor_local(states).cpu().data.numpy()\n",
    "            \n",
    "        self.actor_local.train()\n",
    "        \n",
    "        actions += self.noise.sample()\n",
    "            \n",
    "        return np.clip(actions, -1, 1)\n",
    "\n",
    "    \n",
    "    def reset(self):\n",
    "        self.noise.reset()\n",
    "\n",
    "        \n",
    "    def learn(self, experiences):\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        \n",
    "        # Critic Update\n",
    "        # --------------------------------------------------\n",
    "        actions_next = self.actor_target(next_states)\n",
    "        \n",
    "        Q_targets_next = self.critic_target(next_states, actions_next)\n",
    "        Q_targets      = rewards + (self.gamma * Q_targets_next * (1 - dones))\n",
    "        Q_expected     = self.critic_local(states, actions)\n",
    "        \n",
    "        critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        \n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        \n",
    "        # Actor Update\n",
    "        # --------------------------------------------------\n",
    "        actions_pred =  self.actor_local(states)\n",
    "        actor_loss   = -self.critic_local(states, actions_pred).mean()\n",
    "\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        \n",
    "        # Target Update\n",
    "        # --------------------------------------------------\n",
    "        self.soft_update(self.critic_local, self.critic_target)\n",
    "        self.soft_update(self.actor_local,  self.actor_target)\n",
    "\n",
    "        \n",
    "    def soft_update(self, local_model, target_model):\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(self.tau * local_param.data + (1.0 - self.tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "# Environement \n",
    "# --------------------------------------------------\n",
    "SEED   = 42\n",
    "ENV_FP = f\"Tennis_Windows_x86_64/Tennis.exe\"\n",
    "ENV    = UnityEnvironment(file_name = ENV_FP)\n",
    "\n",
    "\n",
    "# Brain \n",
    "# --------------------------------------------------\n",
    "BRAIN_NAME = ENV.brain_names[0]\n",
    "BRAIN      = ENV.brains[BRAIN_NAME]\n",
    "\n",
    "\n",
    "# Environment Data \n",
    "# --------------------------------------------------\n",
    "ENV_INFO    = ENV.reset(train_mode = True)[BRAIN_NAME]\n",
    "N_AGENTS    = len(ENV_INFO.agents)\n",
    "ACTION_SIZE = BRAIN.vector_action_space_size\n",
    "STATE_SIZE  = ENV_INFO.vector_observations.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training on CUDA\n"
     ]
    }
   ],
   "source": [
    "AGENT = Agent(\n",
    "    state_size  = STATE_SIZE, \n",
    "    action_size = ACTION_SIZE, \n",
    "    n_agents    = N_AGENTS,\n",
    "    fc1         = 256, \n",
    "    fc2         = 128, \n",
    "    leakiness   = 1e-2,\n",
    "    actor_lr    = 1e-4,\n",
    "    critic_lr   = 3e-4,\n",
    "    buffer_size = 1000000,\n",
    "    batch_size  = 256,\n",
    "    gamma       = 0.99,\n",
    "    tau         = 1e-3,\n",
    "    decay       = 1e-4,\n",
    "    mu          = 0.0, \n",
    "    theta       = 0.15, \n",
    "    sigma       = 0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Training Scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddpg(n_episodes = 2000, max_t = 1000, epochs = 1000):\n",
    "    window_size   = 100\n",
    "    scores_window = deque(maxlen = window_size) \n",
    "    max_scores    = [] \n",
    "    moving_avgs   = []\n",
    "    \n",
    "    for i_episode in range(1, epochs + 1):\n",
    "        env_info       = ENV.reset(train_mode = True)[BRAIN_NAME]\n",
    "        states         = env_info.vector_observations\n",
    "        episode_scores = np.zeros(N_AGENTS) \n",
    "        \n",
    "        AGENT.reset()\n",
    "\n",
    "        for t in range(max_t):\n",
    "            actions     = AGENT.act(states)\n",
    "            env_info    = ENV.step(actions)[BRAIN_NAME]\n",
    "            next_states = env_info.vector_observations\n",
    "            rewards     = env_info.rewards\n",
    "            dones       = env_info.local_done\n",
    "            \n",
    "            for state, action, reward, next_state, done in zip(states, actions, rewards, next_states, dones):\n",
    "                AGENT.step(state, action, reward, next_state, done, t)\n",
    "            \n",
    "            episode_scores += np.array(rewards)\n",
    "            states          = next_states\n",
    "            \n",
    "            if np.any(dones):\n",
    "                break\n",
    "                \n",
    "        best_score = np.max(episode_scores)\n",
    "        \n",
    "        max_scores.append(best_score)\n",
    "        scores_window.append(best_score)\n",
    "        \n",
    "        avg_score = np.mean(scores_window)\n",
    "        \n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}\\tMax Episode Score: {:.2f}'.format(i_episode, avg_score, best_score), end=\"\")\n",
    "        \n",
    "        if i_episode % 10 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}\\tMax Episode Score: {:.2f}'.format(i_episode, avg_score, best_score))\n",
    "\n",
    "        if avg_score >= 0.5:\n",
    "            print(\"\\nEnvironment solved in {} episodes!\\tAverage score: {:.2f}\".format(i_episode - window_size, avg_score))\n",
    "            \n",
    "            torch.save(AGENT.actor_local.state_dict(),  os.path.join(\"checkpoints\", \"checkpoint_actor.pth\")) \n",
    "            torch.save(AGENT.critic_local.state_dict(), os.path.join(\"checkpoints\", \"checkpoint_critic.pth\"))\n",
    "            \n",
    "            break\n",
    "\n",
    "    np.save('scores/scores.npy', max_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10\tAverage Score: 0.02\tMax Episode Score: 0.09\n",
      "Episode 20\tAverage Score: 0.01\tMax Episode Score: 0.00\n",
      "Episode 30\tAverage Score: 0.01\tMax Episode Score: 0.00\n",
      "Episode 40\tAverage Score: 0.01\tMax Episode Score: 0.09\n",
      "Episode 50\tAverage Score: 0.01\tMax Episode Score: 0.10\n",
      "Episode 60\tAverage Score: 0.01\tMax Episode Score: 0.00\n",
      "Episode 70\tAverage Score: 0.02\tMax Episode Score: 0.00\n",
      "Episode 80\tAverage Score: 0.02\tMax Episode Score: 0.10\n",
      "Episode 90\tAverage Score: 0.03\tMax Episode Score: 0.00\n",
      "Episode 100\tAverage Score: 0.02\tMax Episode Score: 0.00\n",
      "Episode 110\tAverage Score: 0.02\tMax Episode Score: 0.00\n",
      "Episode 120\tAverage Score: 0.02\tMax Episode Score: 0.00\n",
      "Episode 130\tAverage Score: 0.03\tMax Episode Score: 0.00\n",
      "Episode 140\tAverage Score: 0.04\tMax Episode Score: 0.10\n",
      "Episode 150\tAverage Score: 0.04\tMax Episode Score: 0.00\n",
      "Episode 160\tAverage Score: 0.03\tMax Episode Score: 0.00\n",
      "Episode 170\tAverage Score: 0.03\tMax Episode Score: 0.00\n",
      "Episode 180\tAverage Score: 0.04\tMax Episode Score: 0.10\n",
      "Episode 190\tAverage Score: 0.03\tMax Episode Score: 0.00\n",
      "Episode 200\tAverage Score: 0.05\tMax Episode Score: 0.10\n",
      "Episode 210\tAverage Score: 0.05\tMax Episode Score: 0.10\n",
      "Episode 220\tAverage Score: 0.06\tMax Episode Score: 0.09\n",
      "Episode 230\tAverage Score: 0.07\tMax Episode Score: 0.20\n",
      "Episode 240\tAverage Score: 0.07\tMax Episode Score: 0.10\n",
      "Episode 250\tAverage Score: 0.08\tMax Episode Score: 0.10\n",
      "Episode 260\tAverage Score: 0.10\tMax Episode Score: 0.10\n",
      "Episode 270\tAverage Score: 0.13\tMax Episode Score: 0.10\n",
      "Episode 280\tAverage Score: 0.14\tMax Episode Score: 0.09\n",
      "Episode 290\tAverage Score: 0.19\tMax Episode Score: 0.00\n",
      "Episode 300\tAverage Score: 0.20\tMax Episode Score: 0.10\n",
      "Episode 310\tAverage Score: 0.23\tMax Episode Score: 0.20\n",
      "Episode 320\tAverage Score: 0.24\tMax Episode Score: 0.29\n",
      "Episode 330\tAverage Score: 0.37\tMax Episode Score: 2.10\n",
      "Episode 340\tAverage Score: 0.50\tMax Episode Score: 2.60\n",
      "Episode 341\tAverage Score: 0.52\tMax Episode Score: 2.60\n",
      "Environment solved in 241 episodes!\tAverage score: 0.52\n"
     ]
    }
   ],
   "source": [
    "ddpg(\n",
    "    n_episodes = 2000, \n",
    "    max_t      = 1000, \n",
    "    epochs     = 1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
